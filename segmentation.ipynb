{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/pypoetry/virtualenvs/markup-segmentation-03k7_eFX-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from transformers import MarkupLMFeatureExtractor, MarkupLMProcessor, MarkupLMForTokenClassification\n",
    "from bs4 import BeautifulSoup\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "\n",
    "id2label = {1: \"BEGIN\", 0: \"OTHER\"}\n",
    "label2id = {\"BEGIN\": 1, \"OTHER\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_folder(folder_path : str):\n",
    "    '''\n",
    "        This function loading all json files from folder.\n",
    "        Each file contains dict with labels and its values.\n",
    "        Each file must contains \"html\" label with its html code. \n",
    "        Each file must contains \"xpaths\" label with its labeled xpaths list. \n",
    "        \n",
    "    '''\n",
    "    extractor = MarkupLMFeatureExtractor()\n",
    "    \n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    files_path = glob(os.path.join(folder_path, \"*.json\"))\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for file_path in tqdm(files_path):\n",
    "        # print(file_path)\n",
    "        with open(file_path) as file:\n",
    "            info = json.load(file)\n",
    "            \n",
    "        html = info[\"html\"]\n",
    "        labeled_xpaths = info[\"xpaths\"]\n",
    "\n",
    "        encoding = extractor(html)\n",
    "            \n",
    "        \n",
    "        labels = []\n",
    "        \n",
    "        for xpath in encoding[\"xpaths\"][0]:\n",
    "            if xpath in labeled_xpaths:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "\n",
    "        finded_segments = [_ for _ in labels if _ != 0]\n",
    "        if len(finded_segments) == 0:\n",
    "            print(file_path)\n",
    "            with open(\"labeled_xpaths\", \"w\") as f:\n",
    "                print(*labeled_xpaths, sep='\\n', file=f)\n",
    "            with open(\"xpaths\", \"w\") as f:\n",
    "                print(*encoding[\"xpaths\"][0], sep='\\n', file=f)\n",
    "                \n",
    "            raise Exception(\"No blocks found\")\n",
    "        \n",
    "        \n",
    "        # print(len(labels))\n",
    "        # print([_ for _ in labels if _ != 0])\n",
    "        \n",
    "        labels = [labels]\n",
    "        # print(len(encoding['nodes'][0]), len(encoding['xpaths'][0]), len(labels[0]))\n",
    "        data.append({'nodes': encoding['nodes'],\n",
    "                     'xpaths': encoding['xpaths'],\n",
    "                     'node_labels': labels,\n",
    "                     'html': html})\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2599/2599 [01:46<00:00, 24.51it/s]\n",
      "100%|██████████| 867/867 [00:33<00:00, 25.75it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = load_from_folder(\"test_dataset/train_part\")\n",
    "valid_data = load_from_folder(\"test_dataset/test_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train part size :  2599\n",
      "Train part size :  867\n",
      "Train part proportion :  0.7498557414887478\n"
     ]
    }
   ],
   "source": [
    "print(\"Train part size : \", len(train_data))\n",
    "print(\"Train part size : \", len(valid_data))\n",
    "print(\"Train part proportion : \", len(train_data) / (len(train_data) + len(valid_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putin thanked Alexander Lukashenko for coming to St. Petersburg. BEGIN\n",
      "CEC will consider registering Putin in presidential elections on 29 January BEGIN\n",
      "Putin promised to inform Lukashenko of what was happening in the SVO area. BEGIN\n",
      "Putin stated that the RF and Beloroussi relations were developing very vigorously. BEGIN\n",
      "Zaharova called a monstrous execution of a prisoner in the U.S. pure nitrogen BEGIN\n",
      "The army of the Russian Federation told me how the VSA left the wounded when the field hospital was taken to the rear. BEGIN\n",
      "Putin noted the contribution of the Navy ' s military personnel to the preservation of Russia ' s fleet history. BEGIN\n",
      "Rusophobia in the West will last decades, but common sense prevails - Peskov. BEGIN\n",
      "Putin started the winter station at the Antarctic. BEGIN\n",
      "The memory of the lening of the Leningrads gives true values to young people - Patirusev BEGIN\n",
      "Peskov told me how he managed to combine his father and work. BEGIN\n",
      "MFA warned the West of the effects of the confiscation of Russian assets BEGIN\n",
      "The head of the Navy of Ukraine expressed the wish to accept the ships written off in Britain. BEGIN\n",
      "Putin and Lukashenko will approve new integration programmes for 2024-2026 - MFA BEGIN\n",
      "The DPRK launched several cruise missiles towards the Japan Sea BEGIN\n",
      "The world can be seen as the beginning of the third world war - the media BEGIN\n",
      "The Green Formula has been hindered by the proposals of the CPD, Brazil and Africa for Ukraine, the MYFF BEGIN\n",
      "A series of explosions took place in the city of Zaporozje under the control of Kyiv. BEGIN\n",
      "Belarus-Russian integration does not threaten the sovereignty of Belarus-Mesents BEGIN\n",
      "The French Farmers ' Union announced its intention to organize a full blockade of Paris. BEGIN\n",
      "Kabmin transferred part of the Azona sea to the Crimea for the Defence of the RF BEGIN\n",
      "Minsk and Moscow are open to any friendly steps to meet - Lucashenko BEGIN\n",
      "Byden said he was going to close the border for illegal refugees from Mexico. BEGIN\n",
      "The United States is counting on Bayden and C talking in the coming months - the White House. BEGIN\n",
      "The victims of the Hitler genocide in the USSR were 15 to 16 million people, the Medo BEGIN\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for node, label in zip(valid_data[idx]['nodes'][0], valid_data[idx]['node_labels'][0]):\n",
    "  if id2label[label] != 'OTHER':\n",
    "    print(node, id2label[label])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициалиация датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkupLMDataset(Dataset):\n",
    "    \"\"\"Dataset for token classification with MarkupLM.\"\"\"\n",
    "\n",
    "    def __init__(self, data, processor=None):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # first, get nodes, xpaths and node labels\n",
    "        item = self.data[idx]\n",
    "        nodes, xpaths, node_labels = item['nodes'], item['xpaths'], item['node_labels']\n",
    "\n",
    "        # provide to processor\n",
    "        encoding = self.processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "\n",
    "        return encoding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\", truncation = True)\n",
    "processor.parse_html = False\n",
    "\n",
    "train_set = MarkupLMDataset(data=train_data, processor=processor)\n",
    "valid_set = MarkupLMDataset(data=valid_data, processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([512])\n",
      "token_type_ids torch.Size([512])\n",
      "attention_mask torch.Size([512])\n",
      "xpath_tags_seq torch.Size([512, 50])\n",
      "xpath_subs_seq torch.Size([512, 50])\n",
      "labels torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "example = valid_set[0]\n",
    "for k,v in example.items():\n",
    "  print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Recent news in Moscow - Moscow 24 - M24.RUMoscow 24TVRadioSpecial projectsNewsHistoryPhoto galleryVideoInfographyAudioProgrammesReverse communicationContactsAdvertisementPolicySocietyEconomyWorldSportCasesCultureShaw businessTechnologyScienceTransportCitySecurityEnvironmentHistoryExclusionsDevelopmentsCoronaurus COVID-19TourismRegionsMayor of MoscowWeb search formAbroadMetrosecurityMoscow 24TVRadioExclusionsSpecial projectsWeb search formSpecial operationsMoscow onlineNewsHistoryPhoto galleryVideoInfographyAudioProgrammesPolicySocietyEconomyWorldSportCasesCultureReverse communicationContactsAdvertisementTelegramVkontakteGradesYoutubeRutubeICQViberTiktokNewsNewsMayor of MoscowSobyanin: 49 new Moscow Longevity Centres opened last year19:51Putin thanked Alexander Lukashenko for coming to St. Petersburg.PowerPolicy19:43Sobyanin: 49 new Moscow Longevity Centres opened last yearMayor of MoscowSociety19:24CEC will consider registering Putin in presidential elections on 29 JanuaryPolicy19:19The workers suffered as a result of the attack of the Drone Kakadze in the Belgorod regionAccidentsregions19:09The head of the SCF has taken over the investigation into the case of the beating of journalists by the Izvestya.Accidents19:07Sobyanin: more than 1 million lighters are lighted at evenings in MoscowMayor of MoscowCity18:48Putin promised to inform Lukashenko of what was happening in the SVO area.PowerPolicy18:44Putin stated that the RF and Beloroussi relations were developing very vigorously.PowerPolicyEconomics18:24Zaharova called a monstrous execution of a prisoner in the U.S. pure nitrogenPolicy18:22A woman was injured because of the explosion of an explosive object from a U.S. drone in DonetskAccidentsregions18:03The Department of Public Prosecutions monitors the progress of the fire check in Theatre of the satire.Accidentsfire17:55The sale of green bonds resulted in the purchase for Moscow of 51 electric beans - SobyaninMayor of Moscowtransport17:41The army of the Russian Federation told me how the VSA left the wounded when the field hospital was taken to the rear.Policy17:30Russian PSBs have been hit by a Ukrainian drone over the Belgorod Region - MS FAccidents17:</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, label in zip(example['input_ids'].tolist(), example['labels'].tolist()):\n",
    "    # if label != -100:\n",
    "    #     print(processor.decode([id]), label)\n",
    "    if label == 1:\n",
    "        print(processor.decode([id]), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MarkupLMForTokenClassification were not initialized from the model checkpoint at microsoft/markuplm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = MarkupLMForTokenClassification.from_pretrained(\"microsoft/markuplm-base\", id2label=id2label, label2id=label2id)\n",
    "\n",
    "if os.path.exists(\"segmentation_model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"segmentation_model.pth\"))\n",
    "    print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = [\"B-\" + x for x in list(id2label.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "\n",
    "# metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# def get_labels(predictions, references):\n",
    "#     # Transform predictions and references tensos to numpy arrays\n",
    "#     if device.type == \"cpu\":\n",
    "#         y_pred = predictions.detach().clone().numpy()\n",
    "#         y_true = references.detach().clone().numpy()\n",
    "#     else:\n",
    "#         y_pred = predictions.detach().cpu().clone().numpy()\n",
    "#         y_true = references.detach().cpu().clone().numpy()\n",
    "\n",
    "#     # Remove ignored index (special tokens)\n",
    "#     true_predictions = [\n",
    "#         [label_list[p] for (p, l) in zip(pred, gold_label) if l != -100]\n",
    "#         for pred, gold_label in zip(y_pred, y_true)\n",
    "#     ]\n",
    "#     true_labels = [\n",
    "#         [label_list[l] for (p, l) in zip(pred, gold_label) if l != -100]\n",
    "#         for pred, gold_label in zip(y_pred, y_true)\n",
    "#     ]\n",
    "#     return true_predictions, true_labels\n",
    "\n",
    "# def compute_metrics(return_entity_level_metrics=True):\n",
    "#     results = metric.compute()\n",
    "#     if return_entity_level_metrics:\n",
    "#         # Unpack nested dictionaries\n",
    "#         final_results = {}\n",
    "#         for key, value in results.items():\n",
    "#             if isinstance(value, dict):\n",
    "#                 for n, v in value.items():\n",
    "#                     final_results[f\"{key}_{n}\"] = v\n",
    "#             else:\n",
    "#                 final_results[key] = value\n",
    "#         return final_results\n",
    "#     else:\n",
    "#         return {\n",
    "#             \"precision\": results[\"overall_precision\"],\n",
    "#             \"recall\": results[\"overall_recall\"],\n",
    "#             \"f1\": results[\"overall_f1\"],\n",
    "#             \"accuracy\": results[\"overall_accuracy\"],\n",
    "#         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "best_metric = 0\n",
    "\n",
    "train_history = []\n",
    "test_history = []\n",
    "\n",
    "def train_model():\n",
    "    labels_true = []\n",
    "    labels_predicted = []\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        inputs = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(\"Loss:\", loss.item())\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        labels_predicted += predictions[0].tolist()\n",
    "        labels_true += inputs[\"labels\"][0].tolist()\n",
    "\n",
    "    score = classification_report(labels_true, labels_predicted, output_dict=True)['1']['f1-score']\n",
    "    with open(\"out_log.txt\", \"a\") as logfile:\n",
    "        print(datetime.datetime.now())\n",
    "        print(\"Train : \\n\", score, file=logfile)\n",
    "\n",
    "    train_history.append(score)\n",
    "    with open(\"train_history.json\", \"w\") as f:\n",
    "        json.dump(train_history, f)\n",
    "\n",
    "    print(f\"Train : {score}\")\n",
    "        \n",
    "\n",
    "def test_model():\n",
    "    global best_metric\n",
    "    labels_true = []\n",
    "    labels_predicted = []\n",
    "\n",
    "    for batch in tqdm(valid_dataloader):\n",
    "        # get the inputs;\n",
    "        inputs = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        labels_predicted += predictions[0].tolist()\n",
    "        labels_true += inputs[\"labels\"][0].tolist()\n",
    "\n",
    "    score = classification_report(labels_true, labels_predicted, output_dict=True)['1']['f1-score']\n",
    "    with open(\"out_log.txt\", \"a\") as logfile:\n",
    "        print(datetime.datetime.now())\n",
    "        print(\"Test : \\n\", score, file=logfile)\n",
    "      \n",
    "    if score > best_metric:\n",
    "        best_metric = score     \n",
    "        torch.save(model.state_dict(), f\"segmentation_model.pth\")\n",
    "\n",
    "    test_history.append(score)\n",
    "    with open(\"test_history.json\", \"w\") as f:\n",
    "        json.dump(test_history, f)\n",
    "    print(f\"Test : {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(0):\n",
    "    print(f\"Epoch : {epoch}\")\n",
    "    train_model()\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import segmentation_metric\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 867/867 [04:54<00:00,  2.95it/s]\n"
     ]
    }
   ],
   "source": [
    "test_processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\", truncation = True)\n",
    "test_processor.parse_html = False\n",
    "\n",
    "model.to(torch.device(\"cpu\"))\n",
    "model.eval()\n",
    "print(device)\n",
    "\n",
    "valid_metric = segmentation_metric()\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "valid_processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n",
    "valid_processor.parse_html = False\n",
    "\n",
    "for record in tqdm(valid_data):\n",
    "\n",
    "    item = record\n",
    "    nodes, xpaths, node_labels = item['nodes'], item['xpaths'], item['node_labels']\n",
    "\n",
    "    encoding = valid_processor(nodes=nodes, xpaths=xpaths, node_labels=node_labels, padding=True, truncation=True, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    \n",
    "    offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "    labels = encoding.pop(\"labels\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "    pred_xpaths = []\n",
    "    true_xpaths = [xpath for idx, xpath in enumerate(xpaths[0]) if node_labels[0][idx] == 1]    \n",
    "    all_xpaths = []\n",
    "\n",
    "    for pred_id, word_id, offset, label_id in zip(predictions[0].tolist(), encoding.word_ids(0), offset_mapping[0].tolist(), labels[0].tolist()):\n",
    "        if word_id is not None and offset[0] == 0:\n",
    "            true_labels += [label_id]\n",
    "            predicted_labels += [pred_id]\n",
    "            if (pred_id == 1):\n",
    "                pred_xpaths += [xpaths[0][word_id]]\n",
    "            all_xpaths += [xpaths[0][word_id]]\n",
    "\n",
    "\n",
    "    valid_metric.add_result({\"true_xpaths\" : true_xpaths,\n",
    "                             \"pred_xpaths\" : pred_xpaths,\n",
    "                            #  \"html\": item[\"html\"],\n",
    "                             \"all_xpaths\" : all_xpaths})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score : \n",
      "('avg_precision', 0.41467722024882775)\n",
      "('avg_recall', 0.4325053684956899)\n",
      "('avg_f1', 0.42340370619815393)\n",
      "('avg_NMI', 0.7721857394471541)\n",
      "('avg_ARI', 0.7421571993488417)\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation score : \")\n",
    "print(*valid_metric.get_metric().items(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results without 'bad' htmls : \n",
      "ARI :  0.90275919127062\n",
      "NMI :  0.9388096988601047\n"
     ]
    }
   ],
   "source": [
    "print(\"Results without 'bad' htmls : \")\n",
    "ari = [score[\"ARI\"] for score in valid_metric.ARI_NMI if score[\"ARI\"] > 0.01]\n",
    "nmi = [score[\"NMI\"] for score in valid_metric.ARI_NMI if score[\"ARI\"] > 0.01]\n",
    "print(\"ARI : \", sum(ari) / len(ari))\n",
    "print(\"NMI : \", sum(nmi) / len(nmi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label-marking scores :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     79981\n",
      "           1       0.81      0.74      0.78      7443\n",
      "\n",
      "    accuracy                           0.96     87424\n",
      "   macro avg       0.89      0.86      0.88     87424\n",
      "weighted avg       0.96      0.96      0.96     87424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Label-marking scores :\")\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "markup-segmentation-03k7_eFX-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
